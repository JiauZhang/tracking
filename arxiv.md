<html>
        <body>
        <p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: The political ideology of conversational AI: Converging evidence on  ChatGPT's pro-environmental, left-libertarian orientation<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01768<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Conversational artificial intelligence (AI) disrupts how humans interact with technology. Recently, OpenAI introduced ChatGPT, a state-of-the-art dialogue model that can converse with its human counterparts with unprecedented capabilities. ChatGPT has witnessed tremendous attention from the media, academia, industry, and the general public, attracting more than a million users within days of its release. However, its explosive adoption for information search and as an automated decision aid underscores the importance to understand its limitations and biases. This paper focuses on one of democratic society's most important decision-making processes: political elections. Prompting ChatGPT with 630 political statements from two leading voting advice applications and the nation-agnostic political compass test in three pre-registered experiments, we uncover ChatGPT's pro-environmental, left-libertarian ideology. For example, ChatGPT would impose taxes on flights, restrict rent increases, and legalize abortion. In the 2021 elections, it would have voted most likely for the Greens both in Germany (B\"undnis 90/Die Gr\"unen) and in the Netherlands (GroenLinks). Our findings are robust when negating the prompts, reversing the order of the statements, varying prompt formality, and across languages (English, German, Dutch, and Spanish). We conclude by discussing the implications of politically biased conversational AI on society. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Infomaxformer: Maximum Entropy Transformer for Long Time-Series  Forecasting Problem<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01772<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The Transformer architecture yields state-of-the-art results in many tasks such as natural language processing (NLP) and computer vision (CV), since the ability to efficiently capture the precise long-range dependency coupling between input sequences. With this advanced capability, however, the quadratic time complexity and high memory usage prevents the Transformer from dealing with long time-series forecasting problem (LTFP). To address these difficulties: (i) we revisit the learned attention patterns of the vanilla self-attention, redesigned the calculation method of self-attention based the Maximum Entropy Principle. (ii) we propose a new method to sparse the self-attention, which can prevent the loss of more important self-attention scores due to random sampling.(iii) We propose Keys/Values Distilling method motivated that a large amount of feature in the original self-attention map is redundant, which can further reduce the time and spatial complexity and make it possible to input longer time-series. Finally, we propose a method that combines the encoder-decoder architecture with seasonal-trend decomposition, i.e., using the encoder-decoder architecture to capture more specific seasonal parts. A large number of experiments on several large-scale datasets show that our Infomaxformer is obviously superior to the existing methods. We expect this to open up a new solution for Transformer to solve LTFP, and exploring the ability of the Transformer architecture to capture much longer temporal dependencies. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: A Protocol for Intelligible Interaction Between Agents That Learn and  Explain<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01819<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Recent engineering developments have seen the emergence of Machine Learning (ML) as a powerful form of data analysis with widespread applicability beyond its historical roots in the design of autonomous agents. However, relatively little attention has been paid to the interaction between people and ML systems. Recent developments on Explainable ML address this by providing visual and textual information on how the ML system arrived at a conclusion. In this paper we view the interaction between humans and ML systems within the broader context of interaction between agents capable of learning and explanation. Within this setting, we argue that it is more helpful to view the interaction as characterised by two-way intelligibility of information rather than once-off explanation of a prediction. We formulate two-way intelligibility as a property of a communication protocol. Development of the protocol is motivated by a set of `Intelligibility Axioms' for decision-support systems that use ML with a human-in-the-loop. The axioms are intended as sufficient criteria to claim that: (a) information provided by a human is intelligible to an ML system; and (b) information provided by an ML system is intelligible to a human. The axioms inform the design of a general synchronous interaction model between agents capable of learning and explanation. We identify conditions of compatibility between agents that result in bounded communication, and define Weak and Strong Two-Way Intelligibility between agents as properties of the communication protocol. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Fragment-based t-SMILES for de novo molecular generation<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01829<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: At present, sequence-based and graph-based models are two of popular used molecular generative models. In this study, we introduce a general-purposed, fragment-based, hierarchical molecular representation named t-SMILES (tree-based SMILES) which describes molecules using a SMILES-type string obtained by doing breadth first search (BFS) on full binary molecular tree formed from fragmented molecular graph. The proposed t-SMILES combines the advantages of graph model paying more attention to molecular topology structure and language model possessing powerful learning ability. Experiments with feature tree rooted JTVAE and chemical reaction-based BRICS molecular decomposing algorithms using sequence-based autoregressive generation models on three popular molecule datasets including Zinc, QM9 and ChEMBL datasets indicate that t-SMILES based models significantly outperform previously proposed fragment-based models and being competitive with classical SMILES based and graph-based approaches. Most importantly, we proposed a new perspective for fragment based molecular designing. Hence, SOTA powerful sequence-based solutions could be easily applied for fragment based molecular tasks. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: When Cyber Aggression Prediction Meets BERT on Social Media<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01877<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Increasingly, cyber aggression becomes the prevalent phenomenon that erodes the social media environment. However, due to subjective and expense, the traditional self-reporting questionnaire is hard to be employed in the current cyber area. In this study, we put forward the prediction model for cyber aggression based on the cutting-edge deep learning algorithm. Building on 320 active Weibo users' social media activities, we construct basic, dynamic, and content features. We elaborate cyber aggression on three dimensions: social exclusion, malicious humour, and guilt induction. We then build the prediction model combined with pretrained BERT model. The empirical evidence shows outperformance and supports a stronger prediction with the BERT model than traditional machine learning models without extra pretrained information. This study offers a solid theoretical model for cyber aggression prediction. Furthermore, this study contributes to cyber aggression behaviors' probing and social media platforms' organization. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Learning Feature Recovery Transformer for Occluded Person  Re-identification<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01879<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: One major issue that challenges person re-identification (Re-ID) is the ubiquitous occlusion over the captured persons. There are two main challenges for the occluded person Re-ID problem, i.e., the interference of noise during feature matching and the loss of pedestrian information brought by the occlusions. In this paper, we propose a new approach called Feature Recovery Transformer (FRT) to address the two challenges simultaneously, which mainly consists of visibility graph matching and feature recovery transformer. To reduce the interference of the noise during feature matching, we mainly focus on visible regions that appear in both images and develop a visibility graph to calculate the similarity. In terms of the second challenge, based on the developed graph similarity, for each query image, we propose a recovery transformer that exploits the feature sets of its $k$-nearest neighbors in the gallery to recover the complete features. Extensive experiments across different person Re-ID datasets, including occluded, partial and holistic datasets, demonstrate the effectiveness of FRT. Specifically, FRT significantly outperforms state-of-the-art results by at least 6.2\% Rank-1 accuracy and 7.2\% mAP scores on the challenging Occluded-Duke dataset. The code is available at https://github.com/xbq1994/Feature-Recovery-Transformer. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: What is in a Text-to-Image Prompt: The Potential of Stable Diffusion in  Visual Arts Education<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01902<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Text-to-Image artificial intelligence (AI) recently saw a major breakthrough with the release of Dall-E and its open-source counterpart, Stable Diffusion. These programs allow anyone to create original visual art pieces by simply providing descriptions in natural language (prompts). Using a sample of 72,980 Stable Diffusion prompts, we propose a formalization of this new medium of art creation and assess its potential for teaching the history of art, aesthetics, and technique. Our findings indicate that text-to-Image AI has the potential to revolutionize the way art is taught, offering new, cost-effective possibilities for experimentation and expression. However, it also raises important questions about the ownership of artistic works. As more and more art is created using these programs, it will be crucial to establish new legal and economic models to protect the rights of artists. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Scalable Communication for Multi-Agent Reinforcement Learning via  Transformer-Based Email Mechanism<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01919<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Communication can impressively improve cooperation in multi-agent reinforcement learning (MARL), especially for partially-observed tasks. However, existing works either broadcast the messages leading to information redundancy, or learn targeted communication by modeling all the other agents as targets, which is not scalable when the number of agents varies. In this work, to tackle the scalability problem of MARL communication for partially-observed tasks, we propose a novel framework Transformer-based Email Mechanism (TEM). The agents adopt local communication to send messages only to the ones that can be observed without modeling all the agents. Inspired by human cooperation with email forwarding, we design message chains to forward information to cooperate with the agents outside the observation range. We introduce Transformer to encode and decode the message chain to choose the next receiver selectively. Empirically, TEM outperforms the baselines on multiple cooperative MARL benchmarks. When the number of agents varies, TEM maintains superior performance without further training. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Auditing citation polarization during the COVID-19 pandemic<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01926<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The recent pandemic stimulated scientists to publish a significant amount of research that created a surge of citations of COVID-19-related papers in a short time, leading to an abrupt inflation of the journal impact factor (IF). By auditing the complete set of COVID-19-related publications in the Web of Science, we reveal here that COVID-19-related research worsened the polarization of academic journals: the IF before the pandemic was proportional to the increment of IF, which had the effect of increasing inequality while retaining the journal rankings. We also found that the most highly cited studies related to COVID-19 were published in prestigious journals at the onset of the epidemic, independent of their innate importance or quality. Through the present quantitative investigation, our findings caution against the belief that quantitative metrics, particularly IF, can indicate the significance of individual papers. Rather, such metrics reflect the social attention given to a particular study. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Learning Trajectory-Word Alignments for Video-Language Tasks<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01953<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Aligning objects with words plays a critical role in Image-Language BERT (IL-BERT) and Video-Language BERT (VDL-BERT). Different from the image case where an object covers some spatial patches, an object in a video usually appears as an object trajectory, i.e., it spans over a few spatial but longer temporal patches and thus contains abundant spatiotemporal contexts. However, modern VDL-BERTs neglect this trajectory characteristic that they usually follow IL-BERTs to deploy the patch-to-word (P2W) attention while such attention may over-exploit trivial spatial contexts and neglect significant temporal contexts. To amend this, we propose a novel TW-BERT to learn Trajectory-Word alignment for solving video-language tasks. Such alignment is learned by a newly designed trajectory-to-word (T2W) attention. Besides T2W attention, we also follow previous VDL-BERTs to set a word-to-patch (W2P) attention in the cross-modal encoder. Since T2W and W2P attentions have diverse structures, our cross-modal encoder is asymmetric. To further help this asymmetric cross-modal encoder build robust vision-language associations, we propose a fine-grained ``align-before-fuse'' strategy to pull close the embedding spaces calculated by the video and text encoders. By the proposed strategy and T2W attention, our TW-BERT achieves SOTA performances on text-to-video retrieval tasks, and comparable performances on video question answering tasks with some VDL-BERTs trained on much more data. The code will be available in the supplementary material. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Adaptively Clustering Neighbor Elements for Image Captioning<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01955<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: We design a novel global-local Transformer named \textbf{Ada-ClustFormer} (\textbf{ACF}) to generate captions. We use this name since each layer of ACF can adaptively cluster input elements to carry self-attention (Self-ATT) for learning local context. Compared with other global-local Transformers which carry Self-ATT in fixed-size windows, ACF can capture varying graininess, \eg, an object may cover different numbers of grids or a phrase may contain diverse numbers of words. To build ACF, we insert a probabilistic matrix C into the Self-ATT layer. For an input sequence {{s}_1,...,{s}_N , C_{i,j} softly determines whether the sub-sequence {s_i,...,s_j} should be clustered for carrying Self-ATT. For implementation, {C}_{i,j} is calculated from the contexts of {{s}_i,...,{s}_j}, thus ACF can exploit the input itself to decide which local contexts should be learned. By using ACF to build the vision encoder and language decoder, the captioning model can automatically discover the hidden structures in both vision and language, which encourages the model to learn a unified structural space for transferring more structural commonalities. The experiment results demonstrate the effectiveness of ACF that we achieve CIDEr of 137.8, which outperforms most SOTA captioning models and achieve comparable scores compared with some BERT-based models. The code will be available in the supplementary material. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: CAT: LoCalization and IdentificAtion Cascade Detection Transformer for  Open-World Object Detection<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01970<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Open-world object detection (OWOD), as a more general and challenging goal, requires the model trained from data on known objects to detect both known and unknown objects and incrementally learn to identify these unknown objects. The existing works which employ standard detection framework and fixed pseudo-labelling mechanism (PLM) have the following problems: (i) The inclusion of detecting unknown objects substantially reduces the model's ability to detect known ones. (ii) The PLM does not adequately utilize the priori knowledge of inputs. (iii) The fixed selection manner of PLM cannot guarantee that the model is trained in the right direction. We observe that humans subconsciously prefer to focus on all foreground objects and then identify each one in detail, rather than localize and identify a single object simultaneously, for alleviating the confusion. This motivates us to propose a novel solution called CAT: LoCalization and IdentificAtion Cascade Detection Transformer which decouples the detection process via the shared decoder in the cascade decoding way. In the meanwhile, we propose the self-adaptive pseudo-labelling mechanism which combines the model-driven with input-driven PLM and self-adaptively generates robust pseudo-labels for unknown objects, significantly improving the ability of CAT to retrieve unknown objects. Comprehensive experiments on two benchmark datasets, i.e., MS-COCO and PASCAL VOC, show that our model outperforms the state-of-the-art in terms of all metrics in the task of OWOD, incremental object detection (IOD) and open-set detection. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Bubble or Not: Measurements, Analyses, and Findings on the Ethereum  ERC721 and ERC1155 Non-fungible Token Ecosystem<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01991<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The non-fungible token (NFT) is an emergent type of cryptocurrency that has garnered extensive attention since its inception. The uniqueness, indivisibility and humanistic value of NFTs are the key characteristics that distinguish them from traditional tokens. The market capitalization of NFT reached 21.5 billion USD in 2021, almost 200 times of all previous transactions. However, the subsequent rapid decline in NFT market fever in the second quarter of 2022 casts doubts on the ostensible boom in the NFT market. To date, there has been no comprehensive and systematic study of the NFT trade market or of the NFT bubble and hype phenomenon. To fill this gap, we conduct an in-depth investigation of the whole Ethereum ERC721 and ERC1155 NFT ecosystem via graph analysis and apply several metrics to measure the characteristics of NFTs. By collecting data from the whole blockchain, we construct three graphs, namely NFT create graph, NFT transfer graph, and NFT hold graph, to characterize the NFT traders, analyze the characteristics of NFTs, and discover many observations and insights. Moreover, we propose new indicators to quantify the activeness and value of NFT and propose an algorithm that combines indicators and graph analyses to find bubble NFTs. Real-world cases demonstrate that our indicators and approach can be used to discern bubble NFTs effectively. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: MS-DINO: Efficient Distributed Training of Vision Transformer Foundation  Model in Medical Domain through Masked Sampling<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.02064<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: In spite of the recent success of deep learning in the medical domain, the problem of data scarcity in the medical domain gets aggravated due to privacy and data ownership issues. Distributed learning approaches including federated learning have been studied to alleviate the problems, but they suffer from cumbersome communication overheads and weakness in privacy protection. To address this, here we propose a self-supervised masked sampling distillation method for vision transformer that can be performed without continuous communication but still enhance privacy using a vision transformer-specific encryption method. The effectiveness of our method is demonstrated with extensive experiments on two medical domain data and two different downstream tasks, showing superior performances than those obtained with the existing distributed learning strategy as well as the fine-tuning only baseline. As the self-supervised model built with the proposed method is capable of having a general semantic understanding of the modality, we demonstrate its potential as a task-agnostic foundation model for various medical tasks, widening the applicability in the medical domain. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: On the Forces of Driver Distraction: Explainable Predictions for the  Visual Demand of In-Vehicle Touchscreen Interactions<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.02065<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: With modern infotainment systems, drivers are increasingly tempted to engage in secondary tasks while driving. Since distracted driving is already one of the main causes of fatal accidents, in-vehicle touchscreen Human-Machine Interfaces (HMIs) must be as little distracting as possible. To ensure that these systems are safe to use, they undergo elaborate and expensive empirical testing, requiring fully functional prototypes. Thus, early-stage methods informing designers about the implication their design may have on driver distraction are of great value. This paper presents a machine learning method that, based on anticipated usage scenarios, predicts the visual demand of in-vehicle touchscreen interactions and provides local and global explanations of the factors influencing drivers' visual attention allocation. The approach is based on large-scale natural driving data continuously collected from production line vehicles and employs the SHapley Additive exPlanation (SHAP) method to provide explanations leveraging informed design decisions. Our approach is more accurate than related work and identifies interactions during which long glances occur with 68 % accuracy and predicts the total glance duration with a mean error of 2.4 s. Our explanations replicate the results of various recent studies and provide fast and easily accessible insights into the effect of UI elements, driving automation, and vehicle speed on driver distraction. The system can not only help designers to evaluate current designs but also help them to better anticipate and understand the implications their design decisions might have on future designs. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Towards Long-Term Time-Series Forecasting: Feature, Pattern, and  Distribution<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.02068<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Long-term time-series forecasting (LTTF) has become a pressing demand in many applications, such as wind power supply planning. Transformer models have been adopted to deliver high prediction capacity because of the high computational self-attention mechanism. Though one could lower the complexity of Transformers by inducing the sparsity in point-wise self-attentions for LTTF, the limited information utilization prohibits the model from exploring the complex dependencies comprehensively. To this end, we propose an efficient Transformerbased model, named Conformer, which differentiates itself from existing methods for LTTF in three aspects: (i) an encoder-decoder architecture incorporating a linear complexity without sacrificing information utilization is proposed on top of sliding-window attention and Stationary and Instant Recurrent Network (SIRN); (ii) a module derived from the normalizing flow is devised to further improve the information utilization by inferring the outputs with the latent variables in SIRN directly; (iii) the inter-series correlation and temporal dynamics in time-series data are modeled explicitly to fuel the downstream self-attention mechanism. Extensive experiments on seven real-world datasets demonstrate that Conformer outperforms the state-of-the-art methods on LTTF and generates reliable prediction results with uncertainty quantification. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Towards Table-to-Text Generation with Pretrained Language Model: A Table  Structure Understanding and Text Deliberating Approach<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.02071<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Although remarkable progress on the neural table-to-text methods has been made, the generalization issues hinder the applicability of these models due to the limited source tables. Large-scale pretrained language models sound like a promising solution to tackle such issues. However, how to effectively bridge the gap between the structured table and the text input by fully leveraging table information to fuel the pretrained model is still not well explored. Besides, another challenge of integrating the deliberation mechanism into the text-to-text pretrained model for solving the table-to-text task remains seldom studied. In this paper, to implement the table-to-text generation with pretrained language model, we propose a table structure understanding and text deliberating approach, namely TASD. Specifically, we devise a three-layered multi-head attention network to realize the table-structure-aware text generation model with the help of the pretrained language model. Furthermore, a multi-pass decoder framework is adopted to enhance the capability of polishing generated text for table descriptions. The empirical studies, as well as human evaluation, on two public datasets, validate that our approach can generate faithful and fluent descriptive texts for different types of tables. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: FICE: Text-Conditioned Fashion Image Editing With Guided GAN Inversion<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.02110<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Fashion-image editing represents a challenging computer vision task, where the goal is to incorporate selected apparel into a given input image. Most existing techniques, known as Virtual Try-On methods, deal with this task by first selecting an example image of the desired apparel and then transferring the clothing onto the target person. Conversely, in this paper, we consider editing fashion images with text descriptions. Such an approach has several advantages over example-based virtual try-on techniques, e.g.: (i) it does not require an image of the target fashion item, and (ii) it allows the expression of a wide variety of visual concepts through the use of natural language. Existing image-editing methods that work with language inputs are heavily constrained by their requirement for training sets with rich attribute annotations or they are only able to handle simple text descriptions. We address these constraints by proposing a novel text-conditioned editing model, called FICE (Fashion Image CLIP Editing), capable of handling a wide variety of diverse text descriptions to guide the editing procedure. Specifically with FICE, we augment the common GAN inversion process by including semantic, pose-related, and image-level constraints when generating images. We leverage the capabilities of the CLIP model to enforce the semantics, due to its impressive image-text association capabilities. We furthermore propose a latent-code regularization technique that provides the means to better control the fidelity of the synthesized images. We validate FICE through rigorous experiments on a combination of VITON images and Fashion-Gen text descriptions and in comparison with several state-of-the-art text-conditioned image editing approaches. Experimental results demonstrate FICE generates highly realistic fashion images and leads to stronger editing performance than existing competing approaches. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Reprogramming Pretrained Language Models for Protein Sequence  Representation Learning<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.02120<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Machine Learning-guided solutions for protein learning tasks have made significant headway in recent years. However, success in scientific discovery tasks is limited by the accessibility of well-defined and labeled in-domain data. To tackle the low-data constraint, recent adaptions of deep learning models pretrained on millions of protein sequences have shown promise; however, the construction of such domain-specific large-scale model is computationally expensive. Here, we propose Representation Learning via Dictionary Learning (R2DL), an end-to-end representation learning framework in which we reprogram deep models for alternate-domain tasks that can perform well on protein property prediction with significantly fewer training samples. R2DL reprograms a pretrained English language model to learn the embeddings of protein sequences, by learning a sparse linear mapping between English and protein sequence vocabulary embeddings. Our model can attain better accuracy and significantly improve the data efficiency by up to $10^5$ times over the baselines set by pretrained and standard supervised methods. To this end, we reprogram an off-the-shelf pre-trained English language transformer and benchmark it on a set of protein physicochemical prediction tasks (secondary structure, stability, homology, stability) as well as on a biomedically relevant set of protein function prediction tasks (antimicrobial, toxicity, antibody affinity). <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Algorithms and Complexity for Computing Nash Equilibria in Adversarial  Team Games<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.02129<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Adversarial team games model multiplayer strategic interactions in which a team of identically-interested players is competing against an adversarial player in a zero-sum game. Such games capture many well-studied settings in game theory, such as congestion games, but go well-beyond to environments wherein the cooperation of one team -- in the absence of explicit communication -- is obstructed by competing entities; the latter setting remains poorly understood despite its numerous applications. Since the seminal work of Von Stengel and Koller (GEB `97), different solution concepts have received attention from an algorithmic standpoint. Yet, the complexity of the standard Nash equilibrium has remained open. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Towards Autoformalization of Mathematics and Code Correctness:  Experiments with Elementary Proofs<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.02195<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The ever-growing complexity of mathematical proofs makes their manual verification by mathematicians very cognitively demanding. Autoformalization seeks to address this by translating proofs written in natural language into a formal representation that is computer-verifiable via interactive theorem provers. In this paper, we introduce a semantic parsing approach, based on the Universal Transformer architecture, that translates elementary mathematical proofs into an equivalent formalization in the language of the Coq interactive theorem prover. The same architecture is also trained to translate simple imperative code decorated with Hoare triples into formally verifiable proofs of correctness in Coq. Experiments on a limited domain of artificial and human-written proofs show that the models generalize well to intermediate lengths not seen during training and variations in natural language. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Skip-Attention: Improving Vision Transformers by Paying Less Attention<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.02240<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: This work aims to improve the efficiency of vision transformers (ViT). While ViTs use computationally expensive self-attention operations in every layer, we identify that these operations are highly correlated across layers -- a key redundancy that causes unnecessary computations. Based on this observation, we propose SkipAt, a method to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers. To ensure that reusing self-attention blocks across layers does not degrade the performance, we introduce a simple parametric function, which outperforms the baseline transformer's performance while running computationally faster. We show the effectiveness of our method in image classification and self-supervised learning on ImageNet-1K, semantic segmentation on ADE20K, image denoising on SIDD, and video denoising on DAVIS. We achieve improved throughput at the same-or-higher accuracy levels in all these tasks. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: TractGraphCNN: anatomically informed graph CNN for classification using  diffusion MRI tractography<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01911<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The structure and variability of the brain's connections can be investigated via prediction of non-imaging phenotypes using neural networks. However, known neuroanatomical relationships between input features are generally ignored in network design. We propose TractGraphCNN, a novel, anatomically informed graph CNN framework for machine learning tasks using diffusion MRI tractography. An EdgeConv module aggregates features from anatomically similar white matter connections indicated by graph edges, and an attention module enables interpretation of predictive white matter tracts. Results in a sex prediction testbed task demonstrate strong performance of TractGraphCNN in two large datasets (HCP and ABCD). Graphs informed by white matter geometry demonstrate higher performance than graphs informed by gray matter connectivity. Overall, the bilateral cingulum and left middle longitudinal fasciculus are consistently highly predictive of sex. This work shows the potential of incorporating anatomical information, especially known anatomical similarities between input features, to guide convolutions in neural networks. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Enabling Augmented Segmentation and Registration in Ultrasound-Guided  Spinal Surgery via Realistic Ultrasound Synthesis from Diagnostic CT Volume<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01940<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: This paper aims to tackle the issues on unavailable or insufficient clinical US data and meaningful annotation to enable bone segmentation and registration for US-guided spinal surgery. While the US is not a standard paradigm for spinal surgery, the scarcity of intra-operative clinical US data is an insurmountable bottleneck in training a neural network. Moreover, due to the characteristics of US imaging, it is difficult to clearly annotate bone surfaces which causes the trained neural network missing its attention to the details. Hence, we propose an In silico bone US simulation framework that synthesizes realistic US images from diagnostic CT volume. Afterward, using these simulated bone US we train a lightweight vision transformer model that can achieve accurate and on-the-fly bone segmentation for spinal sonography. In the validation experiments, the realistic US simulation was conducted by deriving from diagnostic spinal CT volume to facilitate a radiation-free US-guided pedicle screw placement procedure. When it is employed for training bone segmentation task, the Chamfer distance achieves 0.599mm; when it is applied for CT-US registration, the associated bone segmentation accuracy achieves 0.93 in Dice, and the registration accuracy based on the segmented point cloud is 0.13~3.37mm in a complication-free manner. While bone US images exhibit strong echoes at the medium interface, it may enable the model indistinguishable between thin interfaces and bone surfaces by simply relying on small neighborhood information. To overcome these shortcomings, we propose to utilize a Long-range Contrast Learning Module to fully explore the Long-range Contrast between the candidates and their surrounding pixels. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: EPR-Net: Constructing non-equilibrium potential landscape via a  variational force projection formulation<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.01946<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: We present a novel yet simple deep learning approach, dubbed EPR-Net, for constructing the potential landscape of high-dimensional non-equilibrium steady state (NESS) systems. The key idea of our approach is to utilize the fact that the negative potential gradient is the orthogonal projection of the driving force in a weighted Hilbert space with respect to the steady-state distribution. The constructed loss function also coincides with the entropy production rate (EPR) formula in NESS theory. This approach can be extended to dealing with dimensionality reduction and state-dependent diffusion coefficients in a unified fashion. The robustness and effectiveness of the proposed approach are demonstrated by numerical studies of several high-dimensional biophysical models with multi-stability, limit cycle, or strange attractor with non-vanishing noise. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/></body>
        </html>
