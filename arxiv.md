<html>
        <body>
        <p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Artificial Intelligence in Psychology Research<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07267<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Large Language Models have vastly grown in capabilities. One potential application of such AI systems is to support data collection in the social sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project (Klein et al., 2018) with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. For the 10 studies that we could analyse, we collected a total of 10,136 responses, each of which was obtained by running GPT3.5 with the corresponding study's survey inputted as text. We find that our GPT3.5-based sample replicates 30% of the original results as well as 30% of the Many Labs 2 results, although there is heterogeneity in both these numbers (as we replicate some original findings that Many Labs 2 did not and vice versa). We also find that unlike the corresponding human subjects, GPT3.5 answered some survey questions with extreme homogeneity$\unicode{x2013}$with zero variation in different runs' responses$\unicode{x2013}$raising concerns that a hypothetical AI-led future may in certain ways be subject to a diminished diversity of thought. Overall, while our results suggest that Large Language Model psychology studies are feasible, their findings should not be assumed to straightforwardly generalise to the human case. Nevertheless, AI-based data collection may eventually become a viable and economically relevant method in the empirical social sciences, making the understanding of its capabilities and applications central. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: ScatterShot: Interactive In-context Example Curation for Text  Transformation<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07346<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The in-context learning capabilities of LLMs like GPT-3 allow annotators to customize an LLM to their specific tasks with a small number of examples. However, users tend to include only the most obvious patterns when crafting examples, resulting in underspecified in-context functions that fall short on unseen cases. Further, it is hard to know when "enough" examples have been included even for known patterns. In this work, we present ScatterShot, an interactive system for building high-quality demonstration sets for in-context learning. ScatterShot iteratively slices unlabeled data into task-specific patterns, samples informative inputs from underexplored or not-yet-saturated slices in an active learning manner, and helps users label more efficiently with the help of an LLM and the current example set. In simulation studies on two text perturbation scenarios, ScatterShot sampling improves the resulting few-shot functions by 4-5 percentage points over random sampling, with less variance as more examples are added. In a user study, ScatterShot greatly helps users in covering different patterns in the input space and labeling in-context examples more efficiently, resulting in better in-context learning and less user effort. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Constrained Decision Transformer for Offline Safe Reinforcement Learning<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07351<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Safe reinforcement learning (RL) trains a constraint satisfaction policy by interacting with the environment. We aim to tackle a more challenging problem: learning a safe policy from an offline dataset. We study the offline safe RL problem from a novel multi-objective optimization perspective and propose the $\epsilon$-reducible concept to characterize problem difficulties. The inherent trade-offs between safety and task performance inspire us to propose the constrained decision transformer (CDT) approach, which can dynamically adjust the trade-offs during deployment. Extensive experiments show the advantages of the proposed method in learning an adaptive, safe, robust, and high-reward policy. CDT outperforms its variants and strong offline safe RL baselines by a large margin with the same hyperparameters across all tasks, while keeping the zero-shot adaptation capability to different constraint thresholds, making our approach more suitable for real-world RL under constraints. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: PolyFormer: Referring Image Segmentation as Sequential Polygon  Generation<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07387<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: In this work, instead of directly predicting the pixel-level segmentation masks, the problem of referring image segmentation is formulated as sequential polygon generation, and the predicted polygons can be later converted into segmentation masks. This is enabled by a new sequence-to-sequence framework, Polygon Transformer (PolyFormer), which takes a sequence of image patches and text query tokens as input, and outputs a sequence of polygon vertices autoregressively. For more accurate geometric localization, we propose a regression-based decoder, which predicts the precise floating-point coordinates directly, without any coordinate quantization error. In the experiments, PolyFormer outperforms the prior art by a clear margin, e.g., 5.40% and 4.52% absolute improvements on the challenging RefCOCO+ and RefCOCOg datasets. It also shows strong generalization ability when evaluated on the referring video segmentation task without fine-tuning, e.g., achieving competitive 61.5% J&amp;F on the Ref-DAVIS17 dataset. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Score-based Diffusion Models in Function Space<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07400<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Diffusion models have recently emerged as a powerful framework for generative modeling. They consist of a forward process that perturbs input data with Gaussian white noise and a reverse process that learns a score function to generate samples by denoising. Despite their tremendous success, they are mostly formulated on finite-dimensional spaces, e.g. Euclidean, limiting their applications to many domains where the data has a functional form such as in scientific computing and 3D geometric data analysis. In this work, we introduce a mathematically rigorous framework called Denoising Diffusion Operators (DDOs) for training diffusion models in function space. In DDOs, the forward process perturbs input functions gradually using a Gaussian process. The generative process is formulated by integrating a function-valued Langevin dynamic. Our approach requires an appropriate notion of the score for the perturbed data distribution, which we obtain by generalizing denoising score matching to function spaces that can be infinite-dimensional. We show that the corresponding discretized algorithm generates accurate samples at a fixed cost that is independent of the data resolution. We theoretically and numerically verify the applicability of our approach on a set of problems, including generating solutions to the Navier-Stokes equation viewed as the push-forward distribution of forcings from a Gaussian Random Field (GRF). <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Unsupervised physics-informed neural network in reaction-diffusion  biology models (Ulcerative colitis and Crohn's disease cases) A preliminary  study<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07405<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: We propose to explore the potential of physics-informed neural networks (PINNs) in solving a class of partial differential equations (PDEs) used to model the propagation of chronic inflammatory bowel diseases, such as Crohn's disease and ulcerative colitis. An unsupervised approach was privileged during the deep neural network training. Given the complexity of the underlying biological system, characterized by intricate feedback loops and limited availability of high-quality data, the aim of this study is to explore the potential of PINNs in solving PDEs. In addition to providing this exploratory assessment, we also aim to emphasize the principles of reproducibility and transparency in our approach, with a specific focus on ensuring the robustness and generalizability through the use of artificial intelligence. We will quantify the relevance of the PINN method with several linear and non-linear PDEs in relation to biology. However, it is important to note that the final solution is dependent on the initial conditions, chosen boundary conditions, and neural network architectures. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Conversational AI-Powered Design: ChatGPT as Designer, User, and Product<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07406<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The recent advancements in Large Language Models (LLMs), particularly conversational LLMs like ChatGPT, have prompted changes in a range of fields, including design. This study aims to examine the capabilities of ChatGPT in a human-centered design process. To this end, a hypothetical design project was conducted, where ChatGPT was utilized to generate personas, simulate interviews with fictional users, create new design ideas, simulate usage scenarios and conversations between an imaginary prototype and fictional users, and lastly evaluate user experience. The results show that ChatGPT effectively performed the tasks assigned to it as a designer, user, or product, providing mostly appropriate responses. The study does, however, highlight some drawbacks such as forgotten information, partial responses, and a lack of output diversity. The paper explains the potential benefits and limitations of using conversational LLMs in design, discusses its implications, and suggests directions for future research in this rapidly evolving area. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Pose-Oriented Transformer with Uncertainty-Guided Refinement for  2D-to-3D Human Pose Estimation<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07408<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: There has been a recent surge of interest in introducing transformers to 3D human pose estimation (HPE) due to their powerful capabilities in modeling long-term dependencies. However, existing transformer-based methods treat body joints as equally important inputs and ignore the prior knowledge of human skeleton topology in the self-attention mechanism. To tackle this issue, in this paper, we propose a Pose-Oriented Transformer (POT) with uncertainty guided refinement for 3D HPE. Specifically, we first develop novel pose-oriented self-attention mechanism and distance-related position embedding for POT to explicitly exploit the human skeleton topology. The pose-oriented self-attention mechanism explicitly models the topological interactions between body joints, whereas the distance-related position embedding encodes the distance of joints to the root joint to distinguish groups of joints with different difficulties in regression. Furthermore, we present an Uncertainty-Guided Refinement Network (UGRN) to refine pose predictions from POT, especially for the difficult joints, by considering the estimated uncertainty of each joint with uncertainty-guided sampling strategy and self-attention mechanism. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art methods with reduced model parameters on 3D HPE benchmarks such as Human3.6M and MPI-INF-3DHP <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Real-time chaotic video encryption based on multithreaded parallel  confusion and diffusion<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07411<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Due to the strong correlation between adjacent pixels, most image encryption schemes perform multiple rounds of confusion and diffusion to protect the image against attacks. Such operations, however, are time-consuming, cannot meet the real-time requirements of video encryption. Existing works, therefore, realize video encryption by simplifying the encryption process or encrypting specific parts of video frames, which results in lower security compared to image encryption. To solve the problem, this paper proposes a real-time chaotic video encryption strategy based on multithreaded parallel confusion and diffusion. It takes a video as the input, splits the frame into subframes, creates a set of threads to simultaneously perform five rounds of confusion and diffusion operations on corresponding subframes, and efficiently outputs the encrypted frames. The encryption speed evaluation shows that our method significantly improves the confusion and diffusion speed, realizes real-time 480x480, 576x576, and 768x768 24FPS video encryption using Intel Core i5-1135G7, Intel Core i7-8700, and Intel Xeon Gold 6226R, respectively. The statistical and security analysis prove that the deployed cryptosystems have outstanding statistical properties, can resist attacks, channel noise, and data loss. Compared with existing works, to the best of our knowledge, the proposed strategy achieves the fastest encryption speed, and realizes the first real-time chaotic video encryption that reaches the security level of image encryption. In addition, it is suitable for many confusion, diffusion algorithms and can be easily deployed with both hardware and software. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Road Redesign Technique Achieving Enhanced Road Safety by Inpainting  with a Diffusion Model<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07440<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Road infrastructure can affect the occurrence of road accidents. Therefore, identifying roadway features with high accident probability is crucial. Here, we introduce image inpainting that can assist authorities in achieving safe roadway design with minimal intervention in the current roadway structure. Image inpainting is based on inpainting safe roadway elements in a roadway image, replacing accident-prone (AP) features by using a diffusion model. After object-level segmentation, the AP features identified by the properties of accident hotspots are masked by a human operator and safe roadway elements are inpainted. With only an average time of 2 min for image inpainting, the likelihood of an image being classified as an accident hotspot drops by an average of 11.85%. In addition, safe urban spaces can be designed considering human factors of commuters such as gaze saliency. Considering this, we introduce saliency enhancement that suggests chrominance alteration for a safe road view. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: How to Train Your DRAGON: Diverse Augmentation Towards Generalizable  Dense Retrieval<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07452<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the limited model capacity. We contradict this hypothesis and show that a generalizable DR can be trained to achieve high accuracy in both supervised and zero-shot retrieval without increasing model size. In particular, we systematically examine the contrastive learning of DRs, under the framework of Data Augmentation (DA). Our study shows that common DA practices such as query augmentation with generative models and pseudo-relevance label creation using a cross-encoder, are often inefficient and sub-optimal. We hence propose a new DA approach with diverse queries and sources of supervision to progressively train a generalizable DR. As a result, DRAGON, our dense retriever trained with diverse augmentation, is the first BERT-base-sized DR to achieve state-of-the-art effectiveness in both supervised and zero-shot evaluations and even competes with models using more complex late interaction (ColBERTv2 and SPLADE++). <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: ForceFormer: Exploring Social Force and Transformer for Pedestrian  Trajectory Prediction<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07583<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Predicting trajectories of pedestrians based on goal information in highly interactive scenes is a crucial step toward Intelligent Transportation Systems and Autonomous Driving. The challenges of this task come from two key sources: (1) complex social interactions in high pedestrian density scenarios and (2) limited utilization of goal information to effectively associate with past motion information. To address these difficulties, we integrate social forces into a Transformer-based stochastic generative model backbone and propose a new goal-based trajectory predictor called ForceFormer. Differentiating from most prior works that simply use the destination position as an input feature, we leverage the driving force from the destination to efficiently simulate the guidance of a target on a pedestrian. Additionally, repulsive forces are used as another input feature to describe the avoidance action among neighboring pedestrians. Extensive experiments show that our proposed method achieves on-par performance measured by distance errors with the state-of-the-art models but evidently decreases collisions, especially in dense pedestrian scenarios on widely used pedestrian datasets. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Towards Optimal Compression: Joint Pruning and Quantization<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07612<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Compression of deep neural networks has become a necessary stage for optimizing model inference on resource-constrained hardware. This paper presents FITCompress, a method for unifying layer-wise mixed precision quantization and pruning under a single heuristic, as an alternative to neural architecture search and Bayesian-based techniques. FITCompress combines the Fisher Information Metric, and path planning through compression space, to pick optimal configurations given size and operation constraints with single-shot fine-tuning. Experiments on ImageNet validate the method and show that our approach yields a better trade-off between accuracy and efficiency when compared to the baselines. Besides computer vision benchmarks, we experiment with the BERT model on a language understanding task, paving the way towards its optimal compression. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Video Probabilistic Diffusion Models in Projected Latent Space<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07685<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Despite the remarkable progress in deep generative models, synthesizing high-resolution and temporally coherent videos still remains a challenge due to their high-dimensionality and complex temporal dynamics along with large spatial variations. Recent works on diffusion models have shown their potential to solve this challenge, yet they suffer from severe computation- and memory-inefficiency that limit the scalability. To handle this issue, we propose a novel generative model for videos, coined projected latent video diffusion models (PVDM), a probabilistic diffusion model which learns a video distribution in a low-dimensional latent space and thus can be efficiently trained with high-resolution videos under limited resources. Specifically, PVDM is composed of two components: (a) an autoencoder that projects a given video as 2D-shaped latent vectors that factorize the complex cubic structure of video pixels and (b) a diffusion model architecture specialized for our new factorized latent space and the training/sampling procedure to synthesize videos of arbitrary length with a single model. Experiments on popular video generation datasets demonstrate the superiority of PVDM compared with previous video synthesis methods; e.g., PVDM obtains the FVD score of 639.7 on the UCF-101 long video (128 frames) generation benchmark, which improves 1773.4 of the prior state-of-the-art. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Knowledge Enhanced Semantic Communication Receiver<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07727<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: In recent years, with the rapid development of deep learning and natural language processing technologies, semantic communication has become a topic of great interest in the field of communication. Although existing deep learning based semantic communication approaches have shown many advantages, they still do not make sufficient use of prior knowledge. Moreover, most existing semantic communication methods focus on the semantic encoding at the transmitter side, while we believe that the semantic decoding capability of the receiver side should also be concerned. In this paper, we propose a knowledge enhanced semantic communication framework in which the receiver can more actively utilize the prior knowledge in the knowledge base for semantic reasoning and decoding, without extra modifications to the neural network structure of the transmitter. Specifically, we design a transformer-based knowledge extractor to find relevant factual triples for the received noisy signal. Extensive simulation results on the WebNLG dataset demonstrate that the proposed receiver yields superior performance on top of the knowledge graph enhanced decoding. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Transformer models: an introduction and catalog<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07730<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: In the past few years we have seen the meteoric appearance of dozens of models of the Transformer family, all of which have funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovation in Transformer models. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews  on Social Media<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07731<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Recent advances in generative models such as GPT may be used to fabricate indistinguishable fake customer reviews at a much lower cost, thus posing challenges for social media platforms to detect these machine-generated fake reviews. We propose to leverage the high-quality elite restaurant reviews verified by Yelp to generate fake reviews from the OpenAI GPT review creator and ultimately fine-tune a GPT output detector to predict fake reviews that significantly outperforms existing solutions. We further apply the model to predict non-elite reviews and identify the patterns across several dimensions, such as review, user and restaurant characteristics, and writing style. We show that social media platforms are continuously challenged by machine-generated fake reviews, although they may implement detection systems to filter out suspicious reviews. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: TFormer: A Transmission-Friendly ViT Model for IoT Devices<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07734<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Deploying high-performance vision transformer (ViT) models on ubiquitous Internet of Things (IoT) devices to provide high-quality vision services will revolutionize the way we live, work, and interact with the world. Due to the contradiction between the limited resources of IoT devices and resource-intensive ViT models, the use of cloud servers to assist ViT model training has become mainstream. However, due to the larger number of parameters and floating-point operations (FLOPs) of the existing ViT models, the model parameters transmitted by cloud servers are large and difficult to run on resource-constrained IoT devices. To this end, this paper proposes a transmission-friendly ViT model, TFormer, for deployment on resource-constrained IoT devices with the assistance of a cloud server. The high performance and small number of model parameters and FLOPs of TFormer are attributed to the proposed hybrid layer and the proposed partially connected feed-forward network (PCS-FFN). The hybrid layer consists of nonlearnable modules and a pointwise convolution, which can obtain multitype and multiscale features with only a few parameters and FLOPs to improve the TFormer performance. The PCS-FFN adopts group convolution to reduce the number of parameters. The key idea of this paper is to propose TFormer with few model parameters and FLOPs to facilitate applications running on resource-constrained IoT devices to benefit from the high performance of the ViT models. Experimental results on the ImageNet-1K, MS COCO, and ADE20K datasets for image classification, object detection, and semantic segmentation tasks demonstrate that the proposed model outperforms other state-of-the-art models. Specifically, TFormer-S achieves 5% higher accuracy on ImageNet-1K than ResNet18 with 1.4$\times$ fewer parameters and FLOPs. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction  Challenge<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07735<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Previous work has shown that Large Language Models are susceptible to so-called data extraction attacks. This allows an attacker to extract a sample that was contained in the training data, which has massive privacy implications. The construction of data extraction attacks is challenging, current attacks are quite inefficient, and there exists a significant gap in the extraction capabilities of untargeted attacks and memorization. Thus, targeted attacks are proposed, which identify if a given sample from the training data, is extractable from a model. In this work, we apply a targeted data extraction attack to the SATML2023 Language Model Training Data Extraction Challenge. We apply a two-step approach. In the first step, we maximise the recall of the model and are able to extract the suffix for 69% of the samples. In the second step, we use a classifier-based Membership Inference Attack on the generations. Our AutoSklearn classifier achieves a precision of 0.841. The full approach reaches a score of 0.405 recall at a 10% false positive rate, which is an improvement of 34% over the baseline of 0.301. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Is ChatGPT better than Human Annotators? Potential and Limitations of  ChatGPT in Explaining Implicit Hate Speech<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07736<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-generated NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Alloprof: a new French question-answer education dataset and its use in  an information retrieval case study<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07738<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Teachers and students are increasingly relying on online learning resources to supplement the ones provided in school. This increase in the breadth and depth of available resources is a great thing for students, but only provided they are able to find answers to their queries. Question-answering and information retrieval systems have benefited from public datasets to train and evaluate their algorithms, but most of these datasets have been in English text written by and for adults. We introduce a new public French question-answering dataset collected from Alloprof, a Quebec-based primary and high-school help website, containing 29 349 questions and their explanations in a variety of school subjects from 10 368 students, with more than half of the explanations containing links to other questions or some of the 2 596 reference pages on the website. We also present a case study of this dataset in an information retrieval task. This dataset was collected on the Alloprof public forum, with all questions verified for their appropriateness and the explanations verified both for their appropriateness and their relevance to the question. To predict relevant documents, architectures using pre-trained BERT models were fine-tuned and evaluated. This dataset will allow researchers to develop question-answering, information retrieval and other algorithms specifically for the French speaking education context. Furthermore, the range of language proficiency, images, mathematical symbols and spelling mistakes will necessitate algorithms based on a multimodal comprehension. The case study we present as a baseline shows an approach that relies on recent techniques provides an acceptable performance level, but more work is necessary before it can reliably be used and trusted in a production setting. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Data Forensics in Diffusion Models: A Systematic Analysis of Membership  Privacy<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07801<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: In recent years, diffusion models have achieved tremendous success in the field of image generation, becoming the stateof-the-art technology for AI-based image processing applications. Despite the numerous benefits brought by recent advances in diffusion models, there are also concerns about their potential misuse, specifically in terms of privacy breaches and intellectual property infringement. In particular, some of their unique characteristics open up new attack surfaces when considering the real-world deployment of such models. With a thorough investigation of the attack vectors, we develop a systematic analysis of membership inference attacks on diffusion models and propose novel attack methods tailored to each attack scenario specifically relevant to diffusion models. Our approach exploits easily obtainable quantities and is highly effective, achieving near-perfect attack performance (&gt;0.9 AUCROC) in realistic scenarios. Our extensive experiments demonstrate the effectiveness of our method, highlighting the importance of considering privacy and intellectual property risks when using diffusion models in image generation tasks. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Notes on Finite Element Discretization for a Model Convection-Diffusion  Problem<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07809<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: We present recent finite element numerical results on a model convection-diffusion problem in the singular perturbed case when the convection term dominates the problem. We compare the standard Galerkin discretization using the linear element with a saddle point least square discretization that uses quadratic test functions, trying to control and explain the non-physical oscillations of the discrete solutions. We also relate the up-winding Petrov-Galerkin method and the stream-line diffusion discretization method, by emphasizing the resulting linear systems and by comparing appropriate error norms. Some results can be extended to the multidimensional case in order to come up with efficient approximations for more general singular perturbed problems, including convection dominated models. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07817<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Modern methods for vision-centric autonomous driving perception widely adopt the bird's-eye-view (BEV) representation to describe a 3D scene. Despite its better efficiency than voxel representation, it has difficulty describing the fine-grained 3D structure of a scene with a single plane. To address this, we propose a tri-perspective view (TPV) representation which accompanies BEV with two additional perpendicular planes. We model each point in the 3D space by summing its projected features on the three planes. To lift image features to the 3D TPV space, we further propose a transformer-based TPV encoder (TPVFormer) to obtain the TPV features effectively. We employ the attention mechanism to aggregate the image features corresponding to each query in each TPV plane. Experiments show that our model trained with sparse supervision effectively predicts the semantic occupancy for all voxels. We demonstrate for the first time that using only camera inputs can achieve comparable performance with LiDAR-based methods on the LiDAR segmentation task on nuScenes. Code: https://github.com/wzzheng/TPVFormer. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: One-Shot Face Video Re-enactment using Hybrid Latent Spaces of StyleGAN2<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07848<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: While recent research has progressively overcome the low-resolution constraint of one-shot face video re-enactment with the help of StyleGAN's high-fidelity portrait generation, these approaches rely on at least one of the following: explicit 2D/3D priors, optical flow based warping as motion descriptors, off-the-shelf encoders, etc., which constrain their performance (e.g., inconsistent predictions, inability to capture fine facial details and accessories, poor generalization, artifacts). We propose an end-to-end framework for simultaneously supporting face attribute edits, facial motions and deformations, and facial identity control for video generation. It employs a hybrid latent-space that encodes a given frame into a pair of latents: Identity latent, $\mathcal{W}_{ID}$, and Facial deformation latent, $\mathcal{S}_F$, that respectively reside in the $W+$ and $SS$ spaces of StyleGAN2. Thereby, incorporating the impressive editability-distortion trade-off of $W+$ and the high disentanglement properties of $SS$. These hybrid latents employ the StyleGAN2 generator to achieve high-fidelity face video re-enactment at $1024^2$. Furthermore, the model supports the generation of realistic re-enactment videos with other latent-based semantic edits (e.g., beard, age, make-up, etc.). Qualitative and quantitative analyses performed against state-of-the-art methods demonstrate the superiority of the proposed approach. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Big Little Transformer Decoder<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07863<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The recent emergence of Large Language Models based on the Transformer architecture has enabled dramatic advancements in the field of Natural Language Processing. However, these models have long inference latency, which limits their deployment, and which makes them prohibitively expensive for various real-time applications. The inference latency is further exacerbated by autoregressive generative tasks, as models need to run iteratively to generate tokens sequentially without leveraging token-level parallelization. To address this, we propose Big Little Decoder (BiLD), a framework that can improve inference efficiency and latency for a wide range of text generation applications. The BiLD framework contains two models with different sizes that collaboratively generate text. The small model runs autoregressively to generate text with a low inference cost, and the large model is only invoked occasionally to refine the small model's inaccurate predictions in a non-autoregressive manner. To coordinate the small and large models, BiLD introduces two simple yet effective policies: (1) the fallback policy that determines when to hand control over to the large model; and (2) the rollback policy that determines when the large model needs to review and correct the small model's inaccurate predictions. To evaluate our framework across different tasks and models, we apply BiLD to various text generation scenarios encompassing machine translation on IWSLT 2017 De-En and WMT 2014 De-En, summarization on CNN/DailyMail, and language modeling on WikiText-2. On an NVIDIA Titan Xp GPU, our framework achieves a speedup of up to 2.13x without any performance drop, and it achieves up to 2.38x speedup with only ~1 point degradation. Furthermore, our framework is fully plug-and-play as it does not require any training or modifications to model architectures. Our code will be open-sourced. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Denoising Diffusion Probabilistic Models for Robust Image  Super-Resolution in the Wild<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07864<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Diffusion models have shown promising results on single-image super-resolution and other image- to-image translation tasks. Despite this success, they have not outperformed state-of-the-art GAN models on the more challenging blind super-resolution task, where the input images are out of distribution, with unknown degradations. This paper introduces SR3+, a diffusion-based model for blind super-resolution, establishing a new state-of-the-art. To this end, we advocate self-supervised training with a combination of composite, parameterized degradations for self-supervised training, and noise-conditioing augmentation during training and testing. With these innovations, a large-scale convolutional architecture, and large-scale datasets, SR3+ greatly outperforms SR3. It outperforms Real-ESRGAN when trained on the same data, with a DRealSR FID score of 36.82 vs. 37.22, which further improves to FID of 32.37 with larger models, and further still with larger training sets. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Dataset Interfaces: Diagnosing Model Failures Using Controllable  Counterfactual Generation<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07865<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Distribution shifts are a major source of failure of deployed machine learning models. However, evaluating a model's reliability under distribution shifts can be challenging, especially since it may be difficult to acquire counterfactual examples that exhibit a specified shift. In this work, we introduce dataset interfaces: a framework which allows users to scalably synthesize such counterfactual examples from a given dataset. Specifically, we represent each class from the input dataset as a custom token within the text space of a text-to-image diffusion model. By incorporating these tokens into natural language prompts, we can then generate instantiations of objects in that dataset under desired distribution shifts. We demonstrate how applying our framework to the ImageNet dataset enables us to study model behavior across a diverse array of shifts, including variations in background, lighting, and attributes of the objects themselves. Code available at https://github.com/MadryLab/dataset-interfaces. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Learning Performance-Improving Code Edits<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07867<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Target Specific De Novo Design of Drug Candidate Molecules with Graph  Transformer-based Generative Adversarial Networks<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07868<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Discovering novel drug candidate molecules is one of the most fundamental and critical steps in drug development. Generative deep learning models, which create synthetic data given a probability distribution, have been developed with the purpose of picking completely new samples from a partially known space. Generative models offer high potential for designing de novo molecules; however, in order for them to be useful in real-life drug development pipelines, these models should be able to design target-specific molecules, which is the next step in this field. In this study, we propose DrugGEN, for the de novo design of drug candidate molecules that interact with selected target proteins. The proposed system represents compounds and protein structures as graphs and processes them via serially connected two generative adversarial networks comprising graph transformers. DrugGEN is trained using a large dataset of compounds from ChEMBL and target-specific bioactive molecules, to design effective and specific inhibitory molecules against the AKT1 protein, which has critical importance for developing treatments against various types of cancer. On fundamental benchmarks, DrugGEN models have either competitive or better performance against other methods. To assess the target-specific generation performance, we conducted further in silico analysis with molecular docking and deep learning-based bioactivity prediction. Results indicate that de novo molecules have high potential for interacting with the AKT1 protein structure in the level of its native ligand. DrugGEN can be used to design completely novel and effective target-specific drug candidate molecules for any druggable protein, given target features and a dataset of experimental bioactivities. Code base, datasets, results and trained models of DrugGEN are available at https://github.com/HUBioDataLab/DrugGEN <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Confidence Score Based Speaker Adaptation of Conformer Speech  Recognition Systems<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07521<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Speaker adaptation techniques provide a powerful solution to customise automatic speech recognition (ASR) systems for individual users. Practical application of unsupervised model-based speaker adaptation techniques to data intensive end-to-end ASR systems is hindered by the scarcity of speaker-level data and performance sensitivity to transcription errors. To address these issues, a set of compact and data efficient speaker-dependent (SD) parameter representations are used to facilitate both speaker adaptive training and test-time unsupervised speaker adaptation of state-of-the-art Conformer ASR systems. The sensitivity to supervision quality is reduced using a confidence score-based selection of the less erroneous subset of speaker-level adaptation data. Two lightweight confidence score estimation modules are proposed to produce more reliable confidence scores. The data sparsity issue, which is exacerbated by data selection, is addressed by modelling the SD parameter uncertainty using Bayesian learning. Experiments on the benchmark 300-hour Switchboard and the 233-hour AMI datasets suggest that the proposed confidence score-based adaptation schemes consistently outperformed the baseline speaker-independent (SI) Conformer model and conventional non-Bayesian, point estimate-based adaptation using no speaker data selection. Similar consistent performance improvements were retained after external Transformer and LSTM language model rescoring. In particular, on the 300-hour Switchboard corpus, statistically significant WER reductions of 1.0%, 1.3%, and 1.4% absolute (9.5%, 10.9%, and 11.3% relative) were obtained over the baseline SI Conformer on the NIST Hub5'00, RT02, and RT03 evaluation sets respectively. Similar WER reductions of 2.7% and 3.3% absolute (8.9% and 10.2% relative) were also obtained on the AMI development and evaluation sets. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Self-Supervised Learning for Modeling Gamma-ray Variability in Blazars<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2302.07700<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Blazars are active galactic nuclei with relativistic jets pointed almost directly at Earth. Blazars are characterized by strong, apparently stochastic flux variability at virtually all observed wavelengths and timescales, from minutes to years, the physical origin of which is still poorly understood. In the high-energy gamma-ray band, the Large Area Telescope aboard the Fermi space telescope (Fermi-LAT) has conducted regular monitoring of thousands of blazars since 2008. Deep learning can help uncover structure in gamma-ray blazars' complex variability patterns that traditional methods based on parametric statistical modeling or manual feature engineering may miss. In this work, we propose using a self-supervised Transformer encoder architecture to construct an effective representation of blazar gamma-ray variability. Measurement errors, upper limits, and missing data are accommodated using learned encodings. The model predicts a set of quantiles for the flux probability distribution at each time step, an architecture naturally suited for describing data generated by a stochastic process. As a proof of concept for how the model output can be analyzed to extract scientifically relevant information, a preliminary search for weekly-timescale time-reversal asymmetry in gamma-ray blazar light curves was conducted, finding no significant evidence for asymmetry. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/></body>
        </html>
