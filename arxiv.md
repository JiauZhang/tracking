<html>
        <body>
        <p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: GH-Feat: Learning Versatile Generative Hierarchical Features from GANs<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05315<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Recent years witness the tremendous success of generative adversarial networks (GANs) in synthesizing photo-realistic images. GAN generator learns to compose realistic images and reproduce the real data distribution. Through that, a hierarchical visual feature with multi-level semantics spontaneously emerges. In this work we investigate that such a generative feature learned from image synthesis exhibits great potentials in solving a wide range of computer vision tasks, including both generative ones and more importantly discriminative ones. We first train an encoder by considering the pretrained StyleGAN generator as a learned loss function. The visual features produced by our encoder, termed as Generative Hierarchical Features (GH-Feat), highly align with the layer-wise GAN representations, and hence describe the input image adequately from the reconstruction perspective. Extensive experiments support the versatile transferability of GH-Feat across a range of applications, such as image editing, image processing, image harmonization, face verification, landmark detection, layout prediction, image retrieval, etc. We further show that, through a proper spatial expansion, our developed GH-Feat can also facilitate fine-grained semantic segmentation using only a few annotations. Both qualitative and quantitative results demonstrate the appealing performance of GH-Feat. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Blind Judgement: Agent-Based Supreme Court Modelling With GPT<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05327<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: We present a novel Transformer-based multi-agent system for simulating the judicial rulings of the 2010-2016 Supreme Court of the United States. We train nine separate models with the respective authored opinions of each supreme justice active ca. 2015 and test the resulting system on 96 real-world cases. We find our system predicts the decisions of the real-world Supreme Court with better-than-random accuracy. We further find a correlation between model accuracy with respect to individual justices and their alignment between legal conservatism &amp; liberalism. Our methods and results hold significance for researchers interested in using language models to simulate politically-charged discourse between multiple agents. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: TransfQMix: Transformers for Leveraging the Graph Structure of  Multi-Agent Reinforcement Learning Problems<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05334<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Coordination is one of the most difficult aspects of multi-agent reinforcement learning (MARL). One reason is that agents normally choose their actions independently of one another. In order to see coordination strategies emerging from the combination of independent policies, the recent research has focused on the use of a centralized function (CF) that learns each agent's contribution to the team reward. However, the structure in which the environment is presented to the agents and to the CF is typically overlooked. We have observed that the features used to describe the coordination problem can be represented as vertex features of a latent graph structure. Here, we present TransfQMix, a new approach that uses transformers to leverage this latent structure and learn better coordination policies. Our transformer agents perform a graph reasoning over the state of the observable entities. Our transformer Q-mixer learns a monotonic mixing-function from a larger graph that includes the internal and external states of the agents. TransfQMix is designed to be entirely transferable, meaning that same parameters can be used to control and train larger or smaller teams of agents. This enables to deploy promising approaches to save training time and derive general policies in MARL, such as transfer learning, zero-shot transfer, and curriculum learning. We report TransfQMix's performances in the Spread and StarCraft II environments. In both settings, it outperforms state-of-the-art Q-Learning models, and it demonstrates effectiveness in solving problems that other methods can not solve. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Text to Point Cloud Localization with Relation-Enhanced Transformer<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05372<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Automatically localizing a position based on a few natural language instructions is essential for future robots to communicate and collaborate with humans. To approach this goal, we focus on the text-to-point-cloud cross-modal localization problem. Given a textual query, it aims to identify the described location from city-scale point clouds. The task involves two challenges. 1) In city-scale point clouds, similar ambient instances may exist in several locations. Searching each location in a huge point cloud with only instances as guidance may lead to less discriminative signals and incorrect results. 2) In textual descriptions, the hints are provided separately. In this case, the relations among those hints are not explicitly described, leading to difficulties of learning relations. To overcome these two challenges, we propose a unified Relation-Enhanced Transformer (RET) to improve representation discriminability for both point cloud and natural language queries. The core of the proposed RET is a novel Relation-enhanced Self-Attention (RSA) mechanism, which explicitly encodes instance (hint)-wise relations for the two modalities. Moreover, we propose a fine-grained cross-modal matching method to further refine the location predictions in a subsequent instance-hint matching stage. Experimental results on the KITTI360Pose dataset demonstrate that our approach surpasses the previous state-of-the-art method by large margin. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: MaNLP@SMM4H22: BERT for Classification of Twitter Posts<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05395<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The reported work is our straightforward approach for the shared task Classification of tweets self-reporting age organized by the Social Media Mining for Health Applications (SMM4H) workshop. This literature describes the approach that was used to build a binary classification system, that classifies the tweets related to birthday posts into two classes namely, exact age(positive class) and non-exact age(negative class). We made two submissions with variations in the preprocessing of text which yielded F1 scores of 0.80 and 0.81 when evaluated by the organizers. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: In BLOOM: Creativity and Affinity in Artificial Lyrics and Art<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05402<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: We apply a large multilingual language model (BLOOM-176B) in open-ended generation of Chinese song lyrics, and evaluate the resulting lyrics for coherence and creativity using human reviewers. We find that current computational metrics for evaluating large language model outputs (MAUVE) have limitations in evaluation of creative writing. We note that the human concept of creativity requires lyrics to be both comprehensible and distinctive -- and that humans assess certain types of machine-generated lyrics to score more highly than real lyrics by popular artists. Inspired by the inherently multimodal nature of album releases, we leverage a Chinese-language stable diffusion model to produce high-quality lyric-guided album art, demonstrating a creative approach for an artist seeking inspiration for an album or single. Finally, we introduce the MojimLyrics dataset, a Chinese-language dataset of popular song lyrics for future research. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: It's Just a Matter of Time: Detecting Depression with Time-Enriched  Multimodal Transformers<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05453<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Depression detection from user-generated content on the internet has been a long-lasting topic of interest in the research community, providing valuable screening tools for psychologists. The ubiquitous use of social media platforms lays out the perfect avenue for exploring mental health manifestations in posts and interactions with other users. Current methods for depression detection from social media mainly focus on text processing, and only a few also utilize images posted by users. In this work, we propose a flexible time-enriched multimodal transformer architecture for detecting depression from social media posts, using pretrained models for extracting image and text embeddings. Our model operates directly at the user-level, and we enrich it with the relative time between posts by using time2vec positional embeddings. Moreover, we propose another model variant, which can operate on randomly sampled and unordered sets of posts to be more robust to dataset noise. We show that our method, using EmoBERTa and CLIP embeddings, surpasses other methods on two multimodal datasets, obtaining state-of-the-art results of 0.931 F1 score on a popular multimodal Twitter dataset, and 0.902 F1 score on the only multimodal Reddit dataset. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Explicit Temporal Embedding in Deep Generative Latent Models for  Longitudinal Medical Image Synthesis<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05465<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Medical imaging plays a vital role in modern diagnostics and treatment. The temporal nature of disease or treatment progression often results in longitudinal data. Due to the cost and potential harm, acquiring large medical datasets necessary for deep learning can be difficult. Medical image synthesis could help mitigate this problem. However, until now, the availability of GANs capable of synthesizing longitudinal volumetric data has been limited. To address this, we use the recent advances in latent space-based image editing to propose a novel joint learning scheme to explicitly embed temporal dependencies in the latent space of GANs. This, in contrast to previous methods, allows us to synthesize continuous, smooth, and high-quality longitudinal volumetric data with limited supervision. We show the effectiveness of our approach on three datasets containing different longitudinal dependencies. Namely, modeling a simple image transformation, breathing motion, and tumor regression, all while showing minimal disentanglement. The implementation is made available online at https://github.com/julschoen/Temp-GAN. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Neural Image Compression with a Diffusion-Based Decoder<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05489<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Diffusion probabilistic models have recently achieved remarkable success in generating high quality image and video data. In this work, we build on this class of generative models and introduce a method for lossy compression of high resolution images. The resulting codec, which we call DIffuson-based Residual Augmentation Codec (DIRAC),is the first neural codec to allow smooth traversal of the rate-distortion-perception tradeoff at test time, while obtaining competitive performance with GAN-based methods in perceptual quality. Furthermore, while sampling from diffusion probabilistic models is notoriously expensive, we show that in the compression setting the number of steps can be drastically reduced. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: CLIP the Gap: A Single Domain Generalization Approach for Object  Detection<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05499<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Single Domain Generalization (SDG) tackles the problem of training a model on a single source domain so that it generalizes to any unseen target domain. While this has been well studied for image classification, the literature on SDG object detection remains almost non-existent. To address the challenges of simultaneously learning robust object localization and representation, we propose to leverage a pre-trained vision-language model to introduce semantic domain concepts via textual prompts. We achieve this via a semantic augmentation strategy acting on the features extracted by the detector backbone, as well as a text-based classification loss. Our experiments evidence the benefits of our approach, outperforming by 10% the only existing SDG object detection method, Single-DGOD [49], on their own diverse weather-driving benchmark. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Fractional Diffusion in the full space: decay and regularity<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05503<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: We consider fractional partial differential equations posed on the full space $\R^d$. Using the well-known Caffarelli-Silvestre extension to $\R^d \times \R^+$ as equivalent definition, we derive existence and uniqueness of weak solutions. We show that solutions to a truncated extension problem on $\R^d \times (0,\YY)$ converge to the solution of the original problem as $\YY \rightarrow \infty$. Moreover, we also provide an algebraic rate of decay and derive weighted analytic-type regularity estimates for solutions to the truncated problem. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Discontinuous Jump Behavior of the Energy Conversion in Wind Energy  Systems<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05553<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The power conversion process of a wind turbine can be characterized by a stochastic differential equation (SDE) of the power output conditioned to certain fixed wind speeds. An analogous approach can also be applied to the mechanical loads on a wind turbine, such as generator torque. The constructed SDE consists of the deterministic and stochastic terms, the latter corresponding to the highly fluctuating behavior of the wind turbine. Here we show how advanced stochastic analysis of the noise contribution can be used to show different operating modes of the conversion process of a wind turbine. The parameters of the SDE, known as Kramers-Moyal (KM) coefficients, are estimated directly from the measurement data. Clear evidence is found that both, continuous diffusion noise and discontinuous jump noise are present. The difference in the noise contributions indicates different operational regions. In particular, we observe that the jump character or discontinuity in power production has a significant contribution in the regions where the control system switches strategies. We find that there is a high increase in jump amplitude near the transition to the rated region, and the switching strategies cannot result in a smooth transition. The proposed analysis provides new insights to the control strategies of the wind turbine. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Predictions of photophysical properties of phosphorescent platinum(II)  complexes based on ensemble machine learning approach<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05639<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Phosphorescent metal complexes have been under intense investigations as emissive dopants for energy efficient organic light emitting diodes (OLEDs). Among them, cyclometalated Pt(II) complexes are widespread triplet emitters with color-tunable emissions. To render their practical applications as OLED emitters, it is in great need to develop Pt(II) complexes with high radiative decay rate constant ($k_r$) and photoluminescence (PL) quantum yield. Thus, an efficient and accurate prediction tool is highly desirable. Here, we develop a general protocol for accurate predictions of emission wavelength, radiative decay rate constant, and PL quantum yield for phosphorescent Pt(II) emitters based on the combination of first-principles quantum mechanical method, machine learning (ML) and experimental calibration. A new dataset concerning phosphorescent Pt(II) emitters is constructed, with more than two hundred samples collected from the literature. Features containing pertinent electronic properties of the complexes are chosen. Our results demonstrate that ensemble learning models combined with stacking-based approaches exhibit the best performance, where the values of squared correlation coefficients ($R^2$), mean absolute error (MAE), and root mean square error (RMSE) are 0.96, 7.21 nm and 13.00 nm for emission wavelength prediction, and 0.81, 0.11 and 0.15 for PL quantum yield prediction. For radiative decay rate constant ($k_r$), the obtained value of $R^2$ is 0.67 while MAE and RMSE are 0.21 and 0.25 (both in log scale), respectively. The accuracy of the protocol is further confirmed using 24 recently reported Pt(II) complexes, which demonstrates its reliability for a broad palette of Pt(II) emitters.We expect this protocol will become a valuable tool, accelerating the rational design of novel OLED materials with desired properties. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Natural Language Processing of Aviation Occurrence Reports for Safety  Management<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05663<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Occurrence reporting is a commonly used method in safety management systems to obtain insight in the prevalence of hazards and accident scenarios. In support of safety data analysis, reports are often categorized according to a taxonomy. However, the processing of the reports can require significant effort from safety analysts and a common problem is interrater variability in labeling processes. Also, in some cases, reports are not processed according to a taxonomy, or the taxonomy does not fully cover the contents of the documents. This paper explores various Natural Language Processing (NLP) methods to support the analysis of aviation safety occurrence reports. In particular, the problems studied are the automatic labeling of reports using a classification model, extracting the latent topics in a collection of texts using a topic model and the automatic generation of probable cause texts. Experimental results showed that (i) under the right conditions the labeling of occurrence reports can be effectively automated with a transformer-based classifier, (ii) topic modeling can be useful for finding the topics present in a collection of reports, and (iii) using a summarization model can be a promising direction for generating probable cause texts. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Short-time SSVEP data extension by a novel generative adversarial  networks based framework<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05599<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high transfer rate and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create supplementary synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length window extension, termed as TEGAN. TEGAN transforms short-time SSVEP signals into long-time artificial SSVEP signals. By incorporating a novel U-Net generator architecture and auxiliary classifier into the network design, the TEGAN could produce conditioned features in the synthetic data. Additionally, to regularize the training process of GAN, we introduced a two-stage training strategy and the LeCam-divergence regularization term during the network implementation. The proposed TEGAN was evaluated on two public SSVEP datasets. With the assistance of TEGAN, the performance of traditional frequency recognition methods and deep learning-based methods have been significantly improved under limited calibration data. This study substantiates the feasibility of the proposed method to extend the data length for short-time SSVEP signals to develop a high-performance BCI system. The proposed GAN-based methods have the great potential of shortening the calibration time for various real-world BCI-based applications, while the novelty of our augmentation strategies shed some value light on understanding the subject-invariant properties of SSVEPs. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Backstepping-based tracking control of the vertical gradient freeze  crystal growth process<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.05631<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The vertical gradient freeze crystal growth process is the main technique for the production of high quality compound semiconductors that are vital for today's electronic applications. A simplified model of this process consists of two 1D diffusion equations with free boundaries for the temperatures in crystal and melt. Both phases are coupled via an ordinary differential equation that describes the evolution of the moving solid/liquid interface. The control of the resulting two-phase Stefan problem is the focus of this contribution. A flatness-based feedforward design is combined with a multi-step backstepping approach to obtain a controller that tracks a reference trajectory for the position of the phase boundary. Specifically, based on some preliminary transformations to map the model into a time-variant PDE-ODE system, consecutive decoupling and backstepping transformations are shown to yield a stable closed loop. The tracking controller is validated in a simulation that considers the actual growth of a Gallium arsenide single crystal. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/></body>
        </html>
