<html>
        <body>
        <p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: The moral authority of ChatGPT<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07098<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: ChatGPT is not only fun to chat with, but it also searches information, answers questions, and gives advice. With consistent moral advice, it might improve the moral judgment and decisions of users, who often hold contradictory moral beliefs. Unfortunately, ChatGPT turns out highly inconsistent as a moral advisor. Nonetheless, it influences users' moral judgment, we find in an experiment, even if they know they are advised by a chatting bot, and they underestimate how much they are influenced. Thus, ChatGPT threatens to corrupt rather than improves users' judgment. These findings raise the question of how to ensure the responsible use of ChatGPT and similar AI. Transparency is often touted but seems ineffective. We propose training to improve digital literacy. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Embodied Agents for Efficient Exploration and Smart Scene Description<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07150<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The development of embodied agents that can communicate with humans in natural language has gained increasing interest over the last years, as it facilitates the diffusion of robotic platforms in human-populated environments. As a step towards this objective, in this work, we tackle a setting for visual navigation in which an autonomous agent needs to explore and map an unseen indoor environment while portraying interesting scenes with natural language descriptions. To this end, we propose and evaluate an approach that combines recent advances in visual robotic exploration and image captioning on images generated through agent-environment interaction. Our approach can generate smart scene descriptions that maximize semantic knowledge of the environment and avoid repetitions. Further, such descriptions offer user-understandable insights into the robot's representation of the environment by highlighting the prominent objects and the correlation between them as encountered during the exploration. To quantitatively assess the performance of the proposed approach, we also devise a specific score that takes into account both exploration and description skills. The experiments carried out on both photorealistic simulated environments and real-world ones demonstrate that our approach can effectively describe the robot's point of view during exploration, improving the human-friendly interpretability of its observations. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: A variational autoencoder-based nonnegative matrix factorisation model  for deep dictionary learning<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07272<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Construction of dictionaries using nonnegative matrix factorisation (NMF) has extensive applications in signal processing and machine learning. With the advances in deep learning, training compact and robust dictionaries using deep neural networks, i.e., dictionaries of deep features, has been proposed. In this study, we propose a probabilistic generative model which employs a variational autoencoder (VAE) to perform nonnegative dictionary learning. In contrast to the existing VAE models, we cast the model under a statistical framework with latent variables obeying a Gamma distribution and design a new loss function to guarantee the nonnegative dictionaries. We adopt an acceptance-rejection sampling reparameterization trick to update the latent variables iteratively. We apply the dictionaries learned from VAE-NMF to two signal processing tasks, i.e., enhancement of speech and extraction of muscle synergies. Experimental results demonstrate that VAE-NMF performs better in learning the latent nonnegative dictionaries in comparison with state-of-the-art methods. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: PTA-Det: Point Transformer Associating Point cloud and Image for 3D  Object Detection<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07301<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: In autonomous driving, 3D object detection based on multi-modal data has become an indispensable approach when facing complex environments around the vehicle. During multi-modal detection, LiDAR and camera are simultaneously applied for capturing and modeling. However, due to the intrinsic discrepancies between the LiDAR point and camera image, the fusion of the data for object detection encounters a series of problems. Most multi-modal detection methods perform even worse than LiDAR-only methods. In this investigation, we propose a method named PTA-Det to improve the performance of multi-modal detection. Accompanied by PTA-Det, a Pseudo Point Cloud Generation Network is proposed, which can convert image information including texture and semantic features by pseudo points. Thereafter, through a transformer-based Point Fusion Transition (PFT) module, the features of LiDAR points and pseudo points from image can be deeply fused under a unified point-based representation. The combination of these modules can conquer the major obstacle in feature fusion across modalities and realizes a complementary and discriminative representation for proposal generation. Extensive experiments on the KITTI dataset show the PTA-Det achieves a competitive result and support its effectiveness. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Face Recognition in the age of CLIP &amp; Billion image datasets<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07315<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: CLIP (Contrastive Language-Image Pre-training) models developed by OpenAI have achieved outstanding results on various image recognition and retrieval tasks, displaying strong zero-shot performance. This means that they are able to perform effectively on tasks for which they have not been explicitly trained. Inspired by the success of OpenAI CLIP, a new publicly available dataset called LAION-5B was collected which resulted in the development of open ViT-H/14, ViT-G/14 models that outperform the OpenAI L/14 model. The LAION-5B dataset also released an approximate nearest neighbor index, with a web interface for search &amp; subset creation. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: HSTFormer: Hierarchical Spatial-Temporal Transformers for 3D Human Pose  Estimation<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07322<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Transformer-based approaches have been successfully proposed for 3D human pose estimation (HPE) from 2D pose sequence and achieved state-of-the-art (SOTA) performance. However, current SOTAs have difficulties in modeling spatial-temporal correlations of joints at different levels simultaneously. This is due to the poses' spatial-temporal complexity. Poses move at various speeds temporarily with various joints and body-parts movement spatially. Hence, a cookie-cutter transformer is non-adaptable and can hardly meet the "in-the-wild" requirement. To mitigate this issue, we propose Hierarchical Spatial-Temporal transFormers (HSTFormer) to capture multi-level joints' spatial-temporal correlations from local to global gradually for accurate 3D HPE. HSTFormer consists of four transformer encoders (TEs) and a fusion module. To the best of our knowledge, HSTFormer is the first to study hierarchical TEs with multi-level fusion. Extensive experiments on three datasets (i.e., Human3.6M, MPI-INF-3DHP, and HumanEva) demonstrate that HSTFormer achieves competitive and consistent performance on benchmarks with various scales and difficulties. Specifically, it surpasses recent SOTAs on the challenging MPI-INF-3DHP dataset and small-scale HumanEva dataset, with a highly generalized systematic approach. The code is available at: https://github.com/qianxiaoye825/HSTFormer. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: ViT-AE++: Improving Vision Transformer Autoencoder for Self-supervised  Medical Image Representations<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07382<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Self-supervised learning has attracted increasing attention as it learns data-driven representation from data without annotations. Vision transformer-based autoencoder (ViT-AE) by He et al. (2021) is a recent self-supervised learning technique that employs a patch-masking strategy to learn a meaningful latent space. In this paper, we focus on improving ViT-AE (nicknamed ViT-AE++) for a more effective representation of both 2D and 3D medical images. We propose two new loss functions to enhance the representation during the training stage. The first loss term aims to improve self-reconstruction by considering the structured dependencies and hence indirectly improving the representation. The second loss term leverages contrastive loss to directly optimize the representation from two randomly masked views. As an independent contribution, we extended ViT-AE++ to a 3D fashion for volumetric medical images. We extensively evaluate ViT-AE++ on both natural images and medical images, demonstrating consistent improvement over vanilla ViT-AE and its superiority over other contrastive learning approaches. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: CLIPTER: Looking at the Bigger Picture in Scene Text Recognition<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07464<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Understanding the scene is often essential for reading text in real-world scenarios. However, current scene text recognizers operate on cropped text images, unaware of the bigger picture. In this work, we harness the representative power of recent vision-language models, such as CLIP, to provide the crop-based recognizer with scene, image-level information. Specifically, we obtain a rich representation of the entire image and fuse it with the recognizer word-level features via cross-attention. Moreover, a gated mechanism is introduced that gradually shifts to the context-enriched representation, enabling simply fine-tuning a pretrained recognizer. We implement our model-agnostic framework, named CLIPTER - CLIP Text Recognition, on several leading text recognizers and demonstrate consistent performance gains, achieving state-of-the-art results over multiple benchmarks. Furthermore, an in-depth analysis reveals improved robustness to out-of-vocabulary words and enhanced generalization in low-data regimes. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Image Embedding for Denoising Generative Models<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07485<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Denoising Diffusion models are gaining increasing popularity in the field of generative modeling for several reasons, including the simple and stable training, the excellent generative quality, and the solid probabilistic foundation. In this article, we address the problem of {\em embedding} an image into the latent space of Denoising Diffusion Models, that is finding a suitable ``noisy'' image whose denoising results in the original image. We particularly focus on Denoising Diffusion Implicit Models due to the deterministic nature of their reverse diffusion process. As a side result of our investigation, we gain a deeper insight into the structure of the latent space of diffusion models, opening interesting perspectives on its exploration, the definition of semantic trajectories, and the manipulation/conditioning of encodings for editing purposes. A particularly interesting property highlighted by our research, which is also characteristic of this class of generative models, is the independence of the latent representation from the networks implementing the reverse diffusion process. In other words, a common seed passed to different networks (each trained on the same dataset), eventually results in identical images. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Machine learning techniques for the Schizophrenia diagnosis: A  comprehensive review and future research directions<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07496<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Schizophrenia (SCZ) is a brain disorder where different people experience different symptoms, such as hallucination, delusion, flat-talk, disorganized thinking, etc. In the long term, this can cause severe effects and diminish life expectancy by more than ten years. Therefore, early and accurate diagnosis of SCZ is prevalent, and modalities like structural magnetic resonance imaging (sMRI), functional MRI (fMRI), diffusion tensor imaging (DTI), and electroencephalogram (EEG) assist in witnessing the brain abnormalities of the patients. Moreover, for accurate diagnosis of SCZ, researchers have used machine learning (ML) algorithms for the past decade to distinguish the brain patterns of healthy and SCZ brains using MRI and fMRI images. This paper seeks to acquaint SCZ researchers with ML and to discuss its recent applications to the field of SCZ study. This paper comprehensively reviews state-of-the-art techniques such as ML classifiers, artificial neural network (ANN), deep learning (DL) models, methodological fundamentals, and applications with previous studies. The motivation of this paper is to benefit from finding the research gaps that may lead to the development of a new model for accurate SCZ diagnosis. The paper concludes with the research finding, followed by the future scope that directly contributes to new research directions. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Graphix-T5: Mixing Pre-Trained Transformers with Graph-Aware Layers for  Text-to-SQL Parsing<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07507<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The task of text-to-SQL parsing, which aims at converting natural language questions into executable SQL queries, has garnered increasing attention in recent years, as it can assist end users in efficiently extracting vital information from databases without the need for technical background. One of the major challenges in text-to-SQL parsing is domain generalization, i.e., how to generalize well to unseen databases. Recently, the pre-trained text-to-text transformer model, namely T5, though not specialized for text-to-SQL parsing, has achieved state-of-the-art performance on standard benchmarks targeting domain generalization. In this work, we explore ways to further augment the pre-trained T5 model with specialized components for text-to-SQL parsing. Such components are expected to introduce structural inductive bias into text-to-SQL parsers thus improving model's capacity on (potentially multi-hop) reasoning, which is critical for generating structure-rich SQLs. To this end, we propose a new architecture GRAPHIX-T5, a mixed model with the standard pre-trained transformer model augmented by some specially-designed graph-aware layers. Extensive experiments and analysis demonstrate the effectiveness of GRAPHIX-T5 across four text-to-SQL benchmarks: SPIDER, SYN, REALISTIC and DK. GRAPHIX-T5 surpass all other T5-based parsers with a significant margin, achieving new state-of-the-art performance. Notably, GRAPHIX-T5-large reach performance superior to the original T5-large by 5.7% on exact match (EM) accuracy and 6.6% on execution accuracy (EX). This even outperforms the T5-3B by 1.2% on EM and 1.5% on EX. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: A Quantitative Exploration of Natural Language Processing Applications  for Electricity Demand Analysis<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07535<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The relationship between electricity demand and weather has been established for a long time and is one of the cornerstones in load prediction for operation and planning, along with behavioral and social aspects such as calendars or significant events. This paper explores how and why the social information contained in the news can be used better to understand aggregate population behaviour in terms of energy demand. The work is done through experiments analysing the impact of predicting features extracted from national news on day-ahead electric demand prediction. The results are compared to a benchmark model trained exclusively on the calendar and meteorological information. Experimental results showed that the best-performing model reduced the official standard errors around 4%, 11%, and 10% in terms of RMSE, MAE, and SMAPE. The best-performing methods are: word frequency identified COVID-19-related keywords; topic distribution that identified news on the pandemic and internal politics; global word embeddings that identified news about international conflicts. This study brings a new perspective to traditional electricity demand analysis and confirms the feasibility of improving its predictions with unstructured information contained in texts, with potential consequences in sociology and economics. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Targeted Image Reconstruction by Sampling Pre-trained Diffusion Model<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07557<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: A trained neural network model contains information on the training data. Given such a model, malicious parties can leverage the "knowledge" in this model and design ways to print out any usable information (known as model inversion attack). Therefore, it is valuable to explore the ways to conduct a such attack and demonstrate its severity. In this work, we proposed ways to generate a data point of the target class without prior knowledge of the exact target distribution by using a pre-trained diffusion model. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Joint Representation Learning for Text and 3D Point Cloud<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07584<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Recent advancements in vision-language pre-training (e.g. CLIP) have shown that vision models can benefit from language supervision. While many models using language modality have achieved great success on 2D vision tasks, the joint representation learning of 3D point cloud with text remains under-explored due to the difficulty of 3D-Text data pair acquisition and the irregularity of 3D data structure. In this paper, we propose a novel Text4Point framework to construct language-guided 3D point cloud models. The key idea is utilizing 2D images as a bridge to connect the point cloud and the language modalities. The proposed Text4Point follows the pre-training and fine-tuning paradigm. During the pre-training stage, we establish the correspondence of images and point clouds based on the readily available RGB-D data and use contrastive learning to align the image and point cloud representations. Together with the well-aligned image and text features achieved by CLIP, the point cloud features are implicitly aligned with the text embeddings. Further, we propose a Text Querying Module to integrate language information into 3D representation learning by querying text embeddings with point cloud features. For fine-tuning, the model learns task-specific 3D representations under informative language guidance from the label set without 2D images. Extensive experiments demonstrate that our model shows consistent improvement on various downstream tasks, such as point cloud semantic segmentation, instance segmentation, and object detection. The code will be available here: https://github.com/LeapLabTHU/Text4Point <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation,  and Detection<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07597<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The introduction of ChatGPT has garnered widespread attention in both academic and industrial communities. ChatGPT is able to respond effectively to a wide range of human questions, providing fluent and comprehensive answers that significantly surpass previous public chatbots in terms of security and usefulness. On one hand, people are curious about how ChatGPT is able to achieve such strength and how far it is from human experts. On the other hand, people are starting to worry about the potential negative impacts that large language models (LLMs) like ChatGPT could have on society, such as fake news, plagiarism, and social security issues. In this work, we collected tens of thousands of comparison responses from both human experts and ChatGPT, with questions ranging from open-domain, financial, medical, legal, and psychological areas. We call the collected dataset the Human ChatGPT Comparison Corpus (HC3). Based on the HC3 dataset, we study the characteristics of ChatGPT's responses, the differences and gaps from human experts, and future directions for LLMs. We conducted comprehensive human evaluations and linguistic analyses of ChatGPT-generated content compared with that of humans, where many interesting results are revealed. After that, we conduct extensive experiments on how to effectively detect whether a certain text is generated by ChatGPT or humans. We build three different detection systems, explore several key factors that influence their effectiveness, and evaluate them in different scenarios. The dataset, code, and models are all publicly available at https://github.com/Hello-SimpleAI/chatgpt-comparison-detection. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Revisiting mass-radius relationships for exoplanet populations: a  machine learning insight<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07143<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: The growing number of exoplanet discoveries and advances in machine learning techniques allow us to find, explore, and understand characteristics of these new worlds beyond our Solar System. We analyze the dataset of 762 confirmed exoplanets and eight Solar System planets using efficient machine-learning approaches to characterize their fundamental quantities. By adopting different unsupervised clustering algorithms, the data are divided into two main classes: planets with $\log R_{p}\leq0.91R_{\oplus}$ and $\log M_{p}\leq1.72M_{\oplus}$ as class 1 and those with $\log R_{p}&gt;0.91R_{\oplus}$ and $\log M_{p}&gt;1.72M_{\oplus}$ as class 2. Various regression models are used to reveal correlations between physical parameters and evaluate their performance. We find that planetary mass, orbital period, and stellar mass play preponderant roles in predicting exoplanet radius. The validation metrics (RMSE, MAE, and $R^{2}$) suggest that the Support Vector Regression has, by and large, better performance than other models and is a promising model for obtaining planetary radius. Not only do we improve the prediction accuracy in logarithmic space, but also we derive parametric equations using the M5P and Markov Chain Monte Carlo methods. Planets of class 1 are shown to be consistent with a positive linear mass-radius relation, while for planets of class 2, the planetary radius represents a strong correlation with their host stars' masses. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Scaffold-Based Multi-Objective Drug Candidate Optimization<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07175<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: Multiparameter optimization (MPO) provides a means to assess and balance several variables based on their importance to the overall objective. However, using MPO methods in therapeutic discovery is challenging due to the number of cheminformatics properties required to find an optimal solution. High throughput virtual screening to identify hit candidates produces a large amount of data with conflicting properties. For instance, toxicity and binding affinity can contradict each other and cause improbable levels of toxicity that can lead to adverse effects. Instead of using the exhaustive method of treating each property, multiple properties can be combined into a single MPO score, with weights assigned for each property. This desirability score also lends itself well to ML applications that can use the score in the loss function. In this work, we will discuss scaffold focused graph-based Markov chain monte carlo framework built to generate molecules with optimal properties. This framework trains itself on-the-fly with the MPO score of each iteration of molecules, and is able to work on a greater number of properties and sample the chemical space around a starting scaffold. Results are compared to the chemical Transformer model molGCT to judge performance between graph and natural language processing approaches. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/><p style="color: black; font-size: 14px; text-align: left; visibility: visible;"><b style="visibility: visible;">Title</b>: Inverse problems for a model of biofilm growth<br style="visibility: visible;"/><b style="visibility: visible;">PDF</b>: https://arxiv.org/pdf/2301.07540<br style="visibility: visible;"/></p><section style="margin-right: auto; margin-left: auto; padding: 0.2em; max-width: 100%; box-sizing: border-box; height: 12em; color: rgb(102, 101, 101); font-size: 14px; line-height: 1.3em; overflow: auto; overflow-wrap: break-word !important; background: rgb(240, 240, 240); text-align: left; visibility: visible;">
        <span style="font-size: 12px; visibility: visible;">
        <b style="visibility: visible;">Abstract</b>: A bacterial biofilm is an aggregate of micro-organisms growing fixed onto a solid surface, rather than floating freely in a liquid. Biofilms play a major role in various practical situations such as surgical infections and water treatment. We consider a non-linear PDE model of biofilm growth subject to initial and Dirichlet boundary conditions, and the inverse coefficient problem of recovering the unknown parameters in the model from extra measurements of quantities related to the biofilm and substrate. By addressing and analysing this inverse problem we provide reliable and robust reconstructions of the primary physical quantities of interest represented by the diffusion coefficients of substrate and biofilm, the biomass spreading parameters, the maximum specific consumption and growth rates, the biofilm decay rate and the half saturation constant. We give particular attention to the constant coefficients involved in the leading-part non-linearity, and present a uniqueness proof and some numerical results. In the course of the numerical investigation, we have identified extra data information that enables improving the reconstruction of the eight-parameter set of physical quantities associated to the model of biofilm growth. <br style="visibility: visible;"/></span>
        </section><br style="visibility: visible;"/></body>
        </html>
